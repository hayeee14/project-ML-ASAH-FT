{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c441ff073526469cacf056536136d337": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_99aa0a948aee40f68ec058c396f9914e",
              "IPY_MODEL_66abb6c15a524aa78f9abbd573c4398c",
              "IPY_MODEL_7fc8f67b4f1a4c92a82676e27b3ed5b1"
            ],
            "layout": "IPY_MODEL_6d18dc68c21b4011b7b1fd44dc053590"
          }
        },
        "99aa0a948aee40f68ec058c396f9914e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67ba86397ff64ff59861cdd98683be18",
            "placeholder": "​",
            "style": "IPY_MODEL_1b0abae847b046218042a5bd4e612904",
            "value": "Converting Videos: 100%"
          }
        },
        "66abb6c15a524aa78f9abbd573c4398c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a49a9d0f8c84a33b4a2811ecb3eab63",
            "max": 15,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_92ed54e4a9de4f52976130264549fd2e",
            "value": 15
          }
        },
        "7fc8f67b4f1a4c92a82676e27b3ed5b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71d446532ead404cb730dbe6adb8a23b",
            "placeholder": "​",
            "style": "IPY_MODEL_68cc9820ce554887aa14fd05d04d60f9",
            "value": " 15/15 [00:00&lt;00:00, 1065.27it/s]"
          }
        },
        "6d18dc68c21b4011b7b1fd44dc053590": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67ba86397ff64ff59861cdd98683be18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b0abae847b046218042a5bd4e612904": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a49a9d0f8c84a33b4a2811ecb3eab63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92ed54e4a9de4f52976130264549fd2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "71d446532ead404cb730dbe6adb8a23b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68cc9820ce554887aa14fd05d04d60f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "50819622939e4f1b8bf5dace30804d73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_070db22d906942888849173890fffbe2",
              "IPY_MODEL_91fbee0e9043453394990e7befebae81",
              "IPY_MODEL_5d22809c967441438e1169e113551a72"
            ],
            "layout": "IPY_MODEL_a1f93ec13cd44a679d5ca422e47ef29b"
          }
        },
        "070db22d906942888849173890fffbe2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36845927432b4f2badf8a54df767b4d8",
            "placeholder": "​",
            "style": "IPY_MODEL_7ad1d654bd2f4b4da90ee97aba7a9b2b",
            "value": "AI Transcribing: 100%"
          }
        },
        "91fbee0e9043453394990e7befebae81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47f60f30baeb430494ca3998e3736266",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ca69499e51714a3e8e07cfa18d4ceb71",
            "value": 20
          }
        },
        "5d22809c967441438e1169e113551a72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9ec7a833cad424ab39ded809047d6d2",
            "placeholder": "​",
            "style": "IPY_MODEL_87c8750ae2294312899034fa6bf02f86",
            "value": " 20/20 [01:25&lt;00:00,  3.79s/it]"
          }
        },
        "a1f93ec13cd44a679d5ca422e47ef29b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36845927432b4f2badf8a54df767b4d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ad1d654bd2f4b4da90ee97aba7a9b2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "47f60f30baeb430494ca3998e3736266": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca69499e51714a3e8e07cfa18d4ceb71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d9ec7a833cad424ab39ded809047d6d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87c8750ae2294312899034fa6bf02f86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Proyek Capstone: AI-Powered Interview Assessment System\n",
        "**Tim A25-CS358**\n",
        "\n",
        "- **Muhammad Rayhan**, M262D5Y1357, sebagai PIC Model & Training (Streamlit & Interface)\n",
        "- **Hafiz Putra Mahesta**, M262D5Y0714, sebagai PIC Integrasi,Model STT, & Fitur (Confidence Score)\n",
        "- **Fahri Rasyidin**, M262D5Y0566, sebagai PIC Data & Evaluasi (Dataset, Kunci Jawaban, WER)"
      ],
      "metadata": {
        "id": "bfb-0BgnKO1e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Packages/Library yang Digunakan"
      ],
      "metadata": {
        "id": "9GiUsL9GLNGF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vBArUUN_Tdgq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9d16be9-f734-426d-848f-ac7f0984af4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-1d9o156r\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-1d9o156r\n",
            "  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (0.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (2.9.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (3.5.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper==20250625) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper==20250625) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper==20250625) (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (1.11.1.6)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper==20250625) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper==20250625) (3.0.3)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=1a386f33ae3d7e453eee2898d0ddd212fe2c76d25c868548360949e9160e565e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-r3fag0w3/wheels/c3/03/25/5e0ba78bc27a3a089f137c9f1d92fdfce16d06996c071a016c\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: openai-whisper\n",
            "Successfully installed openai-whisper-20250625\n",
            "Collecting jiwer\n",
            "  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from jiwer) (8.3.1)\n",
            "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
            "  Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Downloading jiwer-4.0.0-py3-none-any.whl (23 kB)\n",
            "Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
            "Successfully installed jiwer-4.0.0 rapidfuzz-3.14.3\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.12/dist-packages (1.0.3)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.12/dist-packages (0.13.1)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.12/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.12/dist-packages (from moviepy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from moviepy) (2.32.4)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.12/dist-packages (from moviepy) (0.1.12)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.12/dist-packages (from moviepy) (2.0.2)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.12/dist-packages (from moviepy) (2.37.2)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from moviepy) (0.6.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa) (3.1.0)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.15.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.1.2)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile) (2.23)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.12/dist-packages (from imageio<3.0,>=2.5->moviepy) (11.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from lazy_loader>=0.1->librosa) (25.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (4.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2025.11.12)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.51.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.5.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow<22,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.12.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.11.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.29.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.51.0-py3-none-any.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m126.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.5.0-py3-none-any.whl (24 kB)\n",
            "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m134.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyngrok, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 pyngrok-7.5.0 streamlit-1.51.0\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install jiwer\n",
        "!pip install moviepy librosa soundfile\n",
        "!pip install streamlit pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import shutil\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import moviepy.editor as mp\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import whisper\n",
        "import jiwer\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from tqdm.notebook import tqdm\n",
        "from pyngrok import ngrok\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "V-Aa70HGjTgl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "939ef897-47c8-4bf0-d729-f069a3ee0600"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/moviepy/config_defaults.py:47: SyntaxWarning: invalid escape sequence '\\P'\n",
            "  IMAGEMAGICK_BINARY = r\"C:\\Program Files\\ImageMagick-6.8.8-Q16\\magick.exe\"\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:294: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  lines_video = [l for l in lines if ' Video: ' in l and re.search('\\d+x\\d+', l)]\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:367: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  rotation_lines = [l for l in lines if 'rotate          :' in l and re.search('\\d+$', l)]\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:370: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  match = re.search('\\d+$', rotation_line)\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBUpY7RV5gpJ",
        "outputId": "fe79b024-6669-4f4a-8582-fa0ced86a204"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Model dan Data"
      ],
      "metadata": {
        "id": "eACBEakarZVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR = \"/content/drive/MyDrive/Dataset\"\n",
        "VIDEO_INPUT_DIR = os.path.join(BASE_DIR, \"Video\")\n",
        "AUDIO_OUTPUT_DIR = os.path.join(BASE_DIR, \"Audio\")\n",
        "GROUND_TRUTH_FILE = os.path.join(BASE_DIR, \"Transkrip_Manual\")\n",
        "\n",
        "# Cek Folder Video\n",
        "if os.path.exists(VIDEO_INPUT_DIR):\n",
        "    video_files = [f for f in os.listdir(VIDEO_INPUT_DIR) if f.lower().endswith(('.mp4', '.webm', '.avi', '.mov', '.mkv'))]\n",
        "    print(f\"[INFO] Folder Video ditemukan.\")\n",
        "    print(f\"[INFO] Jumlah video yang siap diproses: {len(video_files)} file\")\n",
        "else:\n",
        "    print(f\"[WARNING] Folder Video TIDAK ditemukan di: {VIDEO_INPUT_DIR}\")\n",
        "\n",
        "if os.path.exists(GROUND_TRUTH_FILE):\n",
        "    print(f\"File Transkrip Manual (Kunci Jawaban) ditemukan.\")\n",
        "else:\n",
        "    print(f\"File Transkrip Manual tidak ditemukan di: {GROUND_TRUTH_FILE}\")\n",
        "\n",
        "try:\n",
        "    # Opsi lain: 'tiny', 'small', 'medium' (semakin besar semakin lambat tapi akurat)\n",
        "    model = whisper.load_model(\"small\")\n",
        "    print(\"[SUCCESS] Model Whisper berhasil dimuat ke dalam sistem.\")\n",
        "except Exception as e:\n",
        "    print(f\"[ERROR] Gagal memuat model. Detail error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pw5WC7Fwrgpm",
        "outputId": "27041348-c467-4889-adb7-0ccc953b3f6f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Folder Video ditemukan.\n",
            "[INFO] Jumlah video yang siap diproses: 15 file\n",
            "File Transkrip Manual (Kunci Jawaban) ditemukan.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 461M/461M [00:03<00:00, 128MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SUCCESS] Model Whisper berhasil dimuat ke dalam sistem.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing Functions"
      ],
      "metadata": {
        "id": "EBBT-U9cdkem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_video_to_audio(video_path, audio_path):\n",
        "    try:\n",
        "        video_clip = mp.VideoFileClip(video_path)\n",
        "        video_clip.audio.write_audiofile(audio_path, codec='pcm_s16le', verbose=False, logger=None)\n",
        "        video_clip.close()\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def transcribe_audio(audio_path):\n",
        "    try:\n",
        "        # Prompt: Kita suruh AI-nya sendiri untuk membuang filler saat nulis\n",
        "        technical_prompt = (\n",
        "            \"Transcribe strictly in English. Context: Machine Learning interview. \"\n",
        "            \"Keywords: TensorFlow, Scikit-learn, CNN, Dropout, Overfitting, Transfer Learning. \"\n",
        "            \"Do not include filler words like umm, uh, ah.\"\n",
        "        )\n",
        "\n",
        "        result = model.transcribe(\n",
        "            audio_path,\n",
        "            fp16=False,\n",
        "            language=\"en\",\n",
        "            initial_prompt=technical_prompt\n",
        "        )\n",
        "        return result[\"text\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Transkripsi: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def remove_fillers(text):\n",
        "    \"\"\"\n",
        "    Menghapus kata-kata filler umum secara otomatis menggunakan Regex.\n",
        "    Ini solusi 'otomatis' yang kamu cari.\n",
        "    \"\"\"\n",
        "    # Daftar kata filler yang mau dihapus (bisa ditambah)\n",
        "    fillers = [\n",
        "        r\"\\bum\\b\", r\"\\buh\\b\", r\"\\buhh\\b\", r\"\\bah\\b\", r\"\\ber\\b\", r\"\\bhmm\\b\",\n",
        "        r\"\\bmhm\\b\", r\"\\buh-huh\\b\", r\"\\bokay\\b\",\n",
        "        r\"\\byou know\\b\", r\"\\bi mean\\b\", r\"\\bkind of\\b\", r\"\\bsort of\\b\",\n",
        "        r\"\\bso\\b\", r\"\\blike\\b\", r\"\\byeah\\b\", r\"\\bright\\b\",\n",
        "    ]\n",
        "\n",
        "    clean_text = text.lower()\n",
        "    for filler in fillers:\n",
        "        clean_text = re.sub(filler, \"\", clean_text)\n",
        "\n",
        "    # Hapus spasi ganda sisa penghapusan\n",
        "    clean_text = re.sub(r'\\s+', ' ', clean_text).strip()\n",
        "    return clean_text\n",
        "\n",
        "def calculate_metrics(reference_text, hypothesis_text):\n",
        "    if not reference_text or not hypothesis_text:\n",
        "        return {\"wer\": 1.0, \"accuracy\": 0.0}\n",
        "\n",
        "    # 1. Bersihkan tanda baca dasar\n",
        "    transformation = jiwer.Compose([\n",
        "        jiwer.ToLowerCase(),\n",
        "        jiwer.RemovePunctuation(),\n",
        "        jiwer.RemoveMultipleSpaces(),\n",
        "        jiwer.Strip(),\n",
        "    ])\n",
        "\n",
        "    ref_basic = transformation(reference_text)\n",
        "    hyp_basic = transformation(hypothesis_text)\n",
        "\n",
        "    # 2. HAPUS FILLER WORDS (Langkah Tambahan)\n",
        "    ref_clean = remove_fillers(ref_basic)\n",
        "    hyp_clean = remove_fillers(hyp_basic)\n",
        "\n",
        "    # 3. Hitung Akurasi\n",
        "    wer_score = jiwer.wer(ref_clean, hyp_clean)\n",
        "    accuracy = max(0, 1 - wer_score) * 100\n",
        "\n",
        "    return {\"wer\": wer_score, \"accuracy\": round(accuracy, 2)}"
      ],
      "metadata": {
        "id": "pqj4uUmUdpW9"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing Pipeline & Evaluation"
      ],
      "metadata": {
        "id": "efs9kC-we0a5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRANSCRIPT_DIR = os.path.join(BASE_DIR, \"Transkrip_Manual\")\n",
        "\n",
        "# 2. CONVERT VIDEO -> AUDIO\n",
        "video_files = [f for f in os.listdir(VIDEO_INPUT_DIR) if f.lower().endswith(('.mp4', '.avi', '.webm'))]\n",
        "print(f\"\\n[STEP 1] Cek Video: {len(video_files)} file ditemukan.\")\n",
        "\n",
        "for video in tqdm(video_files, desc=\"Converting Videos\"):\n",
        "    v_path = os.path.join(VIDEO_INPUT_DIR, video)\n",
        "    a_path = os.path.join(AUDIO_OUTPUT_DIR, os.path.splitext(video)[0] + \".wav\")\n",
        "\n",
        "    if not os.path.exists(a_path):\n",
        "        convert_video_to_audio(v_path, a_path)\n",
        "\n",
        "# 3. PROSES SEMUA AUDIO (Video + Audio Tambahan)\n",
        "all_audio_files = []\n",
        "\n",
        "for f in os.listdir(AUDIO_OUTPUT_DIR):\n",
        "    if f.lower().endswith('.wav'):\n",
        "        all_audio_files.append(os.path.join(AUDIO_OUTPUT_DIR, f))\n",
        "\n",
        "# Hilangkan duplikat path\n",
        "all_audio_files = list(set(all_audio_files))\n",
        "\n",
        "print(f\"\\n[STEP 2] Total File Audio Siap Proses: {len(all_audio_files)} file\")\n",
        "\n",
        "# 4. LOOP TRANSKRIPSI & EVALUASI\n",
        "processing_results = [] # INI VARIABEL PENTING UNTUK CELL 6\n",
        "total_accuracy = 0\n",
        "count_evaluated = 0\n",
        "\n",
        "for audio_path in tqdm(all_audio_files, desc=\"AI Transcribing\"):\n",
        "    filename = os.path.basename(audio_path)\n",
        "    base_name = os.path.splitext(filename)[0]\n",
        "\n",
        "    # A. Transkrip AI\n",
        "    pred_text = transcribe_audio(audio_path)\n",
        "\n",
        "    # B. Evaluasi vs Manual\n",
        "    metrics = {\"accuracy\": 0.0}\n",
        "    truth_text = \"N/A\"\n",
        "\n",
        "    txt_path = os.path.join(TRANSCRIPT_DIR, base_name + \".txt\")\n",
        "\n",
        "    if os.path.exists(txt_path):\n",
        "        try:\n",
        "            with open(txt_path, 'r', encoding='utf-8') as f:\n",
        "                truth_text = f.read().strip()\n",
        "            # Hitung akurasi\n",
        "            metrics = calculate_metrics(truth_text, pred_text)\n",
        "            total_accuracy += metrics[\"accuracy\"]\n",
        "            count_evaluated += 1\n",
        "        except: pass\n",
        "\n",
        "    # Simpan hasil ke list\n",
        "    processing_results.append({\n",
        "        \"filename\": filename,\n",
        "        \"prediction\": pred_text,\n",
        "        \"ground_truth\": truth_text[:100], # Preview aja\n",
        "        \"accuracy\": metrics[\"accuracy\"]\n",
        "    })\n",
        "\n",
        "# 5. LAPORAN AKHIR\n",
        "print(\"-\" * 50)\n",
        "df_results = pd.DataFrame(processing_results)\n",
        "\n",
        "if count_evaluated > 0:\n",
        "    avg_accuracy = total_accuracy / count_evaluated\n",
        "else:\n",
        "    avg_accuracy = 0.0\n",
        "\n",
        "print(f\"Total Data Diproses       : {len(processing_results)}\")\n",
        "print(f\"Data dengan Kunci Jawaban : {count_evaluated}\")\n",
        "print(f\"Rata-rata Akurasi         : {avg_accuracy:.2f}%\")\n",
        "\n",
        "if avg_accuracy >= 90:\n",
        "    print(\"STATUS: LULUS (Akurasi >= 90%)\")\n",
        "else:\n",
        "    print(\"STATUS: BELUM LULUS\")\n",
        "\n",
        "df_results[[\"filename\", \"accuracy\"]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 897,
          "referenced_widgets": [
            "c441ff073526469cacf056536136d337",
            "99aa0a948aee40f68ec058c396f9914e",
            "66abb6c15a524aa78f9abbd573c4398c",
            "7fc8f67b4f1a4c92a82676e27b3ed5b1",
            "6d18dc68c21b4011b7b1fd44dc053590",
            "67ba86397ff64ff59861cdd98683be18",
            "1b0abae847b046218042a5bd4e612904",
            "7a49a9d0f8c84a33b4a2811ecb3eab63",
            "92ed54e4a9de4f52976130264549fd2e",
            "71d446532ead404cb730dbe6adb8a23b",
            "68cc9820ce554887aa14fd05d04d60f9",
            "50819622939e4f1b8bf5dace30804d73",
            "070db22d906942888849173890fffbe2",
            "91fbee0e9043453394990e7befebae81",
            "5d22809c967441438e1169e113551a72",
            "a1f93ec13cd44a679d5ca422e47ef29b",
            "36845927432b4f2badf8a54df767b4d8",
            "7ad1d654bd2f4b4da90ee97aba7a9b2b",
            "47f60f30baeb430494ca3998e3736266",
            "ca69499e51714a3e8e07cfa18d4ceb71",
            "d9ec7a833cad424ab39ded809047d6d2",
            "87c8750ae2294312899034fa6bf02f86"
          ]
        },
        "id": "QkMpnuiWe2GM",
        "outputId": "089c2d5c-4ad1-49e3-8c50-ec8cf8abe09e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[STEP 1] Cek Video: 15 file ditemukan.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Converting Videos:   0%|          | 0/15 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c441ff073526469cacf056536136d337"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[STEP 2] Total File Audio Siap Proses: 20 file\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "AI Transcribing:   0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "50819622939e4f1b8bf5dace30804d73"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "Total Data Diproses       : 20\n",
            "Data dengan Kunci Jawaban : 20\n",
            "Rata-rata Akurasi         : 95.27%\n",
            "STATUS: LULUS (Akurasi >= 90%)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     filename  accuracy\n",
              "0    interview_question_4.wav     92.31\n",
              "1   interview_question_17.wav     94.16\n",
              "2   interview_question_13.wav    100.00\n",
              "3   interview_question_14.wav    100.00\n",
              "4   interview_question_11.wav     92.98\n",
              "5   interview_question_12.wav     90.65\n",
              "6    interview_question_7.wav     99.08\n",
              "7   interview_question_19.wav     92.67\n",
              "8    interview_question_9.wav     98.30\n",
              "9   interview_question_18.wav     95.93\n",
              "10   interview_question_6.wav     93.53\n",
              "11   interview_question_2.wav     95.21\n",
              "12  interview_question_16.wav     97.73\n",
              "13   interview_question_3.wav     94.77\n",
              "14   interview_question_5.wav     94.20\n",
              "15  interview_question_20.wav     91.11\n",
              "16   interview_question_8.wav    100.00\n",
              "17  interview_question_10.wav     92.26\n",
              "18  interview_question_15.wav    100.00\n",
              "19   interview_question_1.wav     90.58"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-37538f1a-107c-4838-97ac-03d33bf0a4c2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>interview_question_4.wav</td>\n",
              "      <td>92.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>interview_question_17.wav</td>\n",
              "      <td>94.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>interview_question_13.wav</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>interview_question_14.wav</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>interview_question_11.wav</td>\n",
              "      <td>92.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>interview_question_12.wav</td>\n",
              "      <td>90.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>interview_question_7.wav</td>\n",
              "      <td>99.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>interview_question_19.wav</td>\n",
              "      <td>92.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>interview_question_9.wav</td>\n",
              "      <td>98.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>interview_question_18.wav</td>\n",
              "      <td>95.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>interview_question_6.wav</td>\n",
              "      <td>93.53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>interview_question_2.wav</td>\n",
              "      <td>95.21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>interview_question_16.wav</td>\n",
              "      <td>97.73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>interview_question_3.wav</td>\n",
              "      <td>94.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>interview_question_5.wav</td>\n",
              "      <td>94.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>interview_question_20.wav</td>\n",
              "      <td>91.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>interview_question_8.wav</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>interview_question_10.wav</td>\n",
              "      <td>92.26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>interview_question_15.wav</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>interview_question_1.wav</td>\n",
              "      <td>90.58</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-37538f1a-107c-4838-97ac-03d33bf0a4c2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-37538f1a-107c-4838-97ac-03d33bf0a4c2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-37538f1a-107c-4838-97ac-03d33bf0a4c2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-079ebfec-db56-4bec-a2c5-3bb032c1b6de\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-079ebfec-db56-4bec-a2c5-3bb032c1b6de')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-079ebfec-db56-4bec-a2c5-3bb032c1b6de button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df_results[[\\\"filename\\\", \\\"accuracy\\\"]]\",\n  \"rows\": 20,\n  \"fields\": [\n    {\n      \"column\": \"filename\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"interview_question_4.wav\",\n          \"interview_question_10.wav\",\n          \"interview_question_20.wav\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.370191740601868,\n        \"min\": 90.58,\n        \"max\": 100.0,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          92.31,\n          94.16,\n          99.08\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI Assessment & JSON Generation"
      ],
      "metadata": {
        "id": "xfYF2PONqiO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. DATABASE KATA KUNCI LENGKAP (1-20)\n",
        "KEYWORD_DB = {\n",
        "    # Soal Machine Learning (1-9)\n",
        "    1: [\"challenge\", \"overcame\", \"team\", \"disagreement\", \"listen\", \"meeting\", \"risk\"],\n",
        "    2: [\"transfer learning\", \"vgg\", \"resnet\", \"mobilenet\", \"efficient\", \"keras\", \"accuracy\"],\n",
        "    3: [\"model\", \"accuracy\", \"efficiency\", \"layers\", \"dense\", \"dropout\", \"smote\", \"imbalanced\"],\n",
        "    4: [\"dropout\", \"overfitting\", \"training\", \"layer\", \"rate\", \"neural network\", \"epoch\"],\n",
        "    5: [\"cnn\", \"convolutional\", \"pooling\", \"flatten\", \"filters\", \"image\", \"classification\", \"conv2d\"],\n",
        "    6: [\"background\", \"technology\", \"solve\", \"problems\", \"future\", \"career\", \"engineer\"],\n",
        "    7: [\"python\", \"pandas\", \"scikit-learn\", \"tensorflow\", \"pytorch\", \"preprocessing\", \"tools\"],\n",
        "    8: [\"machine learning\", \"learn\", \"data\", \"patterns\", \"predictions\", \"examples\", \"adapt\"],\n",
        "    9: [\"debug\", \"error\", \"check\", \"data\", \"hyperparameters\", \"learning rate\", \"batch size\"],\n",
        "\n",
        "    # Soal Arsitektur & General (10-20)\n",
        "    10: [\"curious\", \"patient\", \"disciplined\", \"problem-solving\", \"adapt\", \"learn\", \"quality\"],\n",
        "    11: [\"creativity\", \"engineering\", \"sustainable\", \"community\", \"design\", \"impact\"],\n",
        "    12: [\"autocad\", \"sketchup\", \"revit\", \"bim\", \"rendering\", \"lumion\", \"3d\", \"software\"],\n",
        "    13: [\"purpose\", \"users\", \"environment\", \"sketches\", \"zoning\", \"flow\", \"light\"],\n",
        "    14: [\"feedback\", \"criticism\", \"collaborative\", \"improve\", \"revise\", \"iteration\", \"open-minded\"],\n",
        "    15: [\"passion\", \"dedication\", \"creativity\", \"analytical\", \"team\", \"value\", \"growth\"],\n",
        "    16: [\"resume\", \"projects\", \"capstone\", \"internship\", \"experience\", \"python\", \"sql\"],\n",
        "    17: [\"challenging\", \"problem\", \"speed\", \"optimization\", \"inference\", \"deploy\", \"solution\"],\n",
        "    18: [\"traveloka\", \"user\", \"product\", \"blog\", \"team\", \"scale\", \"impact\"],\n",
        "    19: [\"teamwork\", \"disagreement\", \"listen\", \"meeting\", \"compromise\", \"result\"],\n",
        "    20: [\"simple\", \"analogy\", \"explain\", \"concept\", \"non-technical\", \"understand\"]\n",
        "}\n",
        "\n",
        "def extract_id_from_filename(filename):\n",
        "    \"\"\"Mengambil angka dari nama file agar urutan benar (1, 2, 3... 10)\"\"\"\n",
        "    numbers = re.findall(r'\\d+', filename)\n",
        "    if numbers:\n",
        "        return int(numbers[0])\n",
        "    return 999\n",
        "\n",
        "def assess_answer_quality(text, question_id):\n",
        "    \"\"\"Menilai jawaban (0-4)\"\"\"\n",
        "    # Validasi input kosong\n",
        "    if not text or len(text) < 10:\n",
        "        return 0, \"Unanswered\"\n",
        "\n",
        "    target_keywords = KEYWORD_DB.get(question_id, [])\n",
        "    # Fallback jika ID tidak ada\n",
        "    if not target_keywords:\n",
        "        target_keywords = [\"experience\", \"project\", \"learn\", \"team\", \"problem\", \"solution\"]\n",
        "\n",
        "    text_lower = text.lower()\n",
        "    hit_count = sum(1 for k in target_keywords if k in text_lower)\n",
        "    word_count = len(text.split())\n",
        "\n",
        "    score = 2\n",
        "    reason = \"General Response with Limited Details.\"\n",
        "\n",
        "    if word_count > 25:\n",
        "        if hit_count >= 3:\n",
        "            score = 4\n",
        "            reason = \"Comprehensive and Very Clear Response (Contains key technical terms).\"\n",
        "        elif hit_count >= 1:\n",
        "            score = 3\n",
        "            reason = \"Specific Explanation with Basic Understanding.\"\n",
        "\n",
        "    return score, reason\n",
        "\n",
        "def generate_final_report(results_data):\n",
        "    final_output = {\n",
        "        \"success\": True,\n",
        "        \"data\": {\n",
        "            \"id\": 131,\n",
        "            \"candidate\": {\n",
        "                \"name\": \"Hafiz Putra Mahesta\",\n",
        "                \"email\": \"hafiz@dicoding.com\",\n",
        "                \"photoUrl\": \"https://path/to/photo.png\"\n",
        "            },\n",
        "            \"assessorProfile\": {\n",
        "                \"id\": 47,\n",
        "                \"name\": \"AI Assessment System\",\n",
        "                \"photoUrl\": \"https://path/to/system_logo.png\"\n",
        "            },\n",
        "            \"reviewedAt\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            \"decision\": \"PASSED\",\n",
        "            \"scoresOverview\": {\"project\": 100, \"interview\": 0, \"total\": 0},\n",
        "            \"reviewChecklistResult\": {\n",
        "                \"project\": [],\n",
        "                \"interviews\": {\n",
        "                    \"minScore\": 0, \"maxScore\": 4, \"scores\": []\n",
        "                }\n",
        "            },\n",
        "            \"Overall notes\": \"Automated assessment by AI System based on Whisper transcription.\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    total_interview_score = 0\n",
        "    count_questions = 0\n",
        "\n",
        "    # 1. PENGURUTAN BENAR (Numerical Sort)\n",
        "    sorted_results = sorted(results_data, key=lambda x: extract_id_from_filename(x['filename']))\n",
        "\n",
        "    for item in sorted_results:\n",
        "        filename = item['filename']\n",
        "        q_id = extract_id_from_filename(filename)\n",
        "\n",
        "        # Penilaian\n",
        "        score, reason = assess_answer_quality(item['prediction'], q_id)\n",
        "\n",
        "        # 2. TRANSKRIP LENGKAP (Full Text)\n",
        "        full_transcript = item['prediction']\n",
        "\n",
        "        checklist_item = {\n",
        "            \"id\": q_id,\n",
        "            \"score\": score,\n",
        "            \"reason\": reason,\n",
        "            \"transcript_preview\": full_transcript\n",
        "        }\n",
        "\n",
        "        final_output[\"data\"][\"reviewChecklistResult\"][\"interviews\"][\"scores\"].append(checklist_item)\n",
        "\n",
        "        # Hitung Total (Hanya ID 1-20)\n",
        "        if 1 <= q_id <= 20:\n",
        "            total_interview_score += score\n",
        "            count_questions += 1\n",
        "\n",
        "    # Finalisasi Skor\n",
        "    if count_questions > 0:\n",
        "        max_possible_score = count_questions * 4\n",
        "        interview_final_score = (total_interview_score / max_possible_score) * 100\n",
        "    else:\n",
        "        interview_final_score = 0\n",
        "\n",
        "    final_output[\"data\"][\"scoresOverview\"][\"interview\"] = round(interview_final_score, 2)\n",
        "\n",
        "    project_score = 100\n",
        "    total_final = (project_score + interview_final_score) / 2\n",
        "    final_output[\"data\"][\"scoresOverview\"][\"total\"] = round(total_final, 2)\n",
        "\n",
        "    final_output[\"data\"][\"decision\"] = \"PASSED\" if total_final >= 75 else \"Need Human Review\"\n",
        "    return final_output\n",
        "\n",
        "# Gunakan data asli jika ada, jika tidak pakai dummy (untuk tes)\n",
        "if 'processing_results' in locals() and processing_results:\n",
        "    data_to_process = processing_results\n",
        "    print(\"[INFO] Menggunakan Data Transkripsi ASLI.\")\n",
        "else:\n",
        "    print(\"[WARNING] Data asli tidak ditemukan. Menggunakan Dummy Data untuk Demo.\")\n",
        "    data_to_process = [\n",
        "        {\"filename\": \"interview_question_1.wav\", \"prediction\": \"Dummy answer 1 full text.\" * 10},\n",
        "        {\"filename\": \"interview_question_10.wav\", \"prediction\": \"Dummy answer 10 full text.\" * 10},\n",
        "        {\"filename\": \"interview_question_2.wav\", \"prediction\": \"Dummy answer 2 full text.\" * 10}\n",
        "    ]\n",
        "\n",
        "json_report = generate_final_report(data_to_process)\n",
        "\n",
        "print(json.dumps(json_report, indent=2))\n",
        "\n",
        "output_json_path = os.path.join(BASE_DIR, \"final_assessment_result.json\")\n",
        "with open(output_json_path, \"w\") as f:\n",
        "    json.dump(json_report, f, indent=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfA18IRoqj0f",
        "outputId": "03a8a341-d7da-4b25-bc8f-52d9d80a3474"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Menggunakan Data Transkripsi ASLI.\n",
            "{\n",
            "  \"success\": true,\n",
            "  \"data\": {\n",
            "    \"id\": 131,\n",
            "    \"candidate\": {\n",
            "      \"name\": \"Hafiz Putra Mahesta\",\n",
            "      \"email\": \"hafiz@dicoding.com\",\n",
            "      \"photoUrl\": \"https://path/to/photo.png\"\n",
            "    },\n",
            "    \"assessorProfile\": {\n",
            "      \"id\": 47,\n",
            "      \"name\": \"AI Assessment System\",\n",
            "      \"photoUrl\": \"https://path/to/system_logo.png\"\n",
            "    },\n",
            "    \"reviewedAt\": \"2025-11-23 17:29:49\",\n",
            "    \"decision\": \"PASSED\",\n",
            "    \"scoresOverview\": {\n",
            "      \"project\": 100,\n",
            "      \"interview\": 98.75,\n",
            "      \"total\": 99.38\n",
            "    },\n",
            "    \"reviewChecklistResult\": {\n",
            "      \"project\": [],\n",
            "      \"interviews\": {\n",
            "        \"minScore\": 0,\n",
            "        \"maxScore\": 4,\n",
            "        \"scores\": [\n",
            "          {\n",
            "            \"id\": 1,\n",
            "            \"score\": 3,\n",
            "            \"reason\": \"Specific Explanation with Basic Understanding.\",\n",
            "            \"transcript_preview\": \"Okay, share any specific challenges you face while working on certification and how not to overcome them. Ah, okay, actually, for the challenges, there are some challenges when I took the certifications, especially for the project submission that I will, I already working with it. The first one is actually to meet, to meet, to meet the specific accuracy or um, violation loss, right, for the evolution matrix. And yeah, actually, that's just need to, to take some trial and error with the, with different architecture. For example, like we can try to add more, uh, layer, more neurons, changes the neurons, or even, uh, I also apply the dropout layer. So, yeah, it really helps with the, the, the violation loss to, to become more lower, right. And yeah, I think that's, that's one of the biggest challenges that, that I have while working on these certifications.\"\n",
            "          },\n",
            "          {\n",
            "            \"id\": 2,\n",
            "            \"score\": 4,\n",
            "            \"reason\": \"Comprehensive and Very Clear Response (Contains key technical terms).\",\n",
            "            \"transcript_preview\": \"Can describe your experience with Transfer Learning and TensorFlow. How do you benefit from projects? Ah, okay. About Transfer Learning is actually we use existing train model from TensorFlow, for example, like VGC 16, VGC 19, right? Especially for some cases that we need to use deep learning using Keras applications. For example, like image classification, we can use Transfer Learning models, which is that's already a train model with exceptionally high accuracy, high performance. Even though it's trained with different data sets, but it really helps to improve our model performance, model accuracy, model loss. And, you know, for example, like MobileNet, VGC 19, VGC 16, yeah, EfficientNet, it will help to improve our models comparing to the one if we use a traditional CNN model, yeah. CNN model with the convolutional 2D, yeah, Max Pulling, and yeah, it's quite good actually to use Transfer Learning. It really helps with our model performance to improve our model performance.\"\n",
            "          },\n",
            "          {\n",
            "            \"id\": 3,\n",
            "            \"score\": 4,\n",
            "            \"reason\": \"Comprehensive and Very Clear Response (Contains key technical terms).\",\n",
            "            \"transcript_preview\": \"Wait, what is this? Describe a complex TensorFlow model you have built and the steps you took to ensure its accuracy and efficiency. Umm, uh, complex TensorFlow model you have built and steps you took to ensure its accuracy. Umm, okay, I will take one of my previous project that I use, yeah, I also use Keras, the TensorFlow model. Uh, it's, it is about, this is prediction, yeah, yeah, this is also, I use the research project for my undergraduate thesis, yeah, for my script C. And, I use this model, it's quite challenging, even though it's achieved high accuracy, yeah, with some dance layer, yeah, with some thrower layer, and trial and error with the, also with the Colbert function, yeah, with the neurons, but the problem is the dataset is not balanced, yeah, so it has the in-balance class datasets, yeah, and the, the process I use is to, yeah, to just to use the technique called SMOPE. SMOPE and, yeah, synthetic over sampling technique with edited nearest neighbor, yeah, which is basically just over sampling and under sampling the datasets, yeah, it helps with the accuracy.\"\n",
            "          },\n",
            "          {\n",
            "            \"id\": 4,\n",
            "            \"score\": 4,\n",
            "            \"reason\": \"Comprehensive and Very Clear Response (Contains key technical terms).\",\n",
            "            \"transcript_preview\": \"Explain how you implement Dropout in, oh, okay, Dropout in, Transfer Flow, and the effect it has on training. Umm, previously I also have implemented the Dropout layer, yeah, also in the project submission within this certifications and we can just add the Dropout layer, for example, if I'm not mistaken, it's, I have used this Dropout layer in the, the one that the case is umm, image classification, yeah, German traffic something if I'm not wrong, I have used this Dropout layer in, in, in the, not in the last, in the middle, in the middle of the layer, so there, there's a flat layer, right, not flatten, the conventional layer, another flatten layer, and I use that Dropout layer, which is, I use with the rate of 0.2 or 0.5 if I, if I'm not wrong, and then the, then slayer and the last, the output layer, right, the effect is, it will, will helps to improve our accuracy and lower our validation loss, by, by turning off some of the previous layer, yeah, for example, like we have a dense layer, 64 and the next layer, we implement the Dropout layer with the rate of 0.5 and it will turn off randomly each epoch, for the previous dense layer, yeah, for the previous dense layer, yeah, for the previous dense layer, yeah, for the previous dense layer,\"\n",
            "          },\n",
            "          {\n",
            "            \"id\": 5,\n",
            "            \"score\": 4,\n",
            "            \"reason\": \"Comprehensive and Very Clear Response (Contains key technical terms).\",\n",
            "            \"transcript_preview\": \"Describe the process of building more conversational, more conversational, powerful image as fiction. Okay. Uh, the CNN run, right? So, uh, at the first time, uh, of course, uh, we need to make sure, uh, there are split, uh, the image folder split for, for, uh, each class. Okay. Uh, and then we can use, uh, Keras per processing, I found a, please take an, uh, image dataset from directory to split the training and the validation dataset. Right. Uh, yeah, of course we can use another, another set, which is the test dataset. Yeah. Uh, but yeah. Okay. The next one, uh, we can, uh, yeah, we can just, uh, maybe we, we need to, implement also the image augmentation. Yeah. Data, uh, data image augmentation to, uh, to make our dataset more, uh, uh, veritative. Right. For example, like we can rotate, we can zoom it, we can crop it. Yeah. Uh, and yeah, the last thing, yeah, of course we, we can build our model with the conventional to be, uh, specify the filters, the carnal size, the activation, of course, the inputs, input shape for the first layer. And then we can apply the next pulling to be, uh, yeah, and the next layer, we can just use conventional to be next pulling and, uh, whatever it is. Uh, and after that, uh, we apply the flatten layer, uh, and dropout layer if you want. And the last thing, uh, don't forget to use the next layer. Right. For the output, like, uh, the last, uh,\"\n",
            "          },\n",
            "          {\n",
            "            \"id\": 6,\n",
            "            \"score\": 4,\n",
            "            \"reason\": \"Comprehensive and Very Clear Response (Contains key technical terms).\",\n",
            "            \"transcript_preview\": \"Ask about yourself and why you want to become AI engineer. My name is Muah Matrean. I came from an IT background and I have been always interested in how technology solves the real problems over time. I become passionate with artificial intelligence because it can run patterns, automatic tests and makes systems much smarter. And I want to become an AI engineer because I enjoy building things that are meaningful and useful. I also love the challenge especially when it comes to data, algorithms and improv-linked models. For me, AI is not just a trend, but it's a long-term skill that will solve the future and also become the part of it. And I also think AI in the future can solve a lot of problems. That's why I am choosing AI to become my professional career.\"\n",
            "          },\n",
            "          {\n",
            "            \"id\": 7,\n",
            "            \"score\": 4,\n",
            "            \"reason\": \"Comprehensive and Very Clear Response (Contains key technical terms).\",\n",
            "            \"transcript_preview\": \"What AI or machine learning tools or skill have you learned so far? I have learned Python as many programming languages and I have practiced using machine learning library such as TensorFlow, PyTorch, SoftSkills, Scikit-learn, and Pandas. I also understand basic concepts like supervised and unsupervised learning mode and model training, evaluation metrics, data processing, and future engineering. Besides that, I have explored some tasks like image classification and text analysis project. And I'm still contentious, improved skills, actually in the building and end-to-end and learning, more advanced AI and machine learning project. And that's the things that I have learned. I want to use power tools for machine learning and AI.\"\n",
            "          },\n",
            "          {\n",
            "            \"id\": 8,\n",
            "            \"score\": 4,\n",
            "            \"reason\": \"Comprehensive and Very Clear Response (Contains key technical terms).\",\n",
            "            \"transcript_preview\": \"Can you explain what is what Machine Learning? Can you explain what Machine Learning is in a simple word? Machine Learning is basically a way for computers to learn from data instead of giving the computers exact instructions. We give it examples and the computers find patterns from the two examples. Once it learns those patterns, it can make predictions by itself. For example, if we give a model many pictures of cats and dogs, it will learn the difference and then be able to identify a new picture it had been seen before. So, Machine Learning is a help computers adapt, improve and make decisions automatically beside on data. I think that's my opinion about how can I explain Machine Learning in a simple word. That's my, that's my question. That, uh, that's it. My explanation about how Machine Learning works in a simple word.\"\n",
            "          },\n",
            "          {\n",
            "            \"id\": 9,\n",
            "            \"score\": 4,\n",
            "            \"reason\": \"Comprehensive and Very Clear Response (Contains key technical terms).\",\n",
            "            \"transcript_preview\": \"Do you usually solve problems when your model does not work well? When a model doesn't work for a well, the first thing I do is check the data, to check the data, because most of the problems usually come from data quality. I look for missing value in correct labels, outliners, or unbalanced classes. After that, I try the different processing steps like a normalised features, selection, or cleaning data set. I have, the issue is not, if the issue is not with the data, then I experiment with a different model. Sometimes a simpler model works better than a complex one. I also tune high parameters such as learning rate, bed size, or number layers to improve performance, another techniques. I use analysing the error case to understand where the model is struggling. Most importantly, I approach the different step by step and avoid changing too many things at the once I document each ad change, so I know what improves the models. The mindset helps me stay calm, systematic, and efficient when solving modern related problems.\"\n",
            "          },\n",
            "          {\n",
            "            \"id\": 10,\n",
            "            \"score\": 4,\n",
            "            \"reason\": \"Comprehensive and Very Clear Response (Contains key technical terms).\",\n",
            "            \"transcript_preview\": \"What personal qualities do you have to support you in becoming AI and Engineer? I believe I have several things that match well in the role of AI Engineer. First, I learn very quick AI is a fast-moving field. So being able to adapt and understand now tools is extremely important. Second, I'm naturally cautious. Like I like to explore how things work, why models behave a certain way, and how small chance can improve performance. I'm also patient and disciplined with self when working on a long technical test like data cleaning, model training, and debugging. Problem solving is one of my strongest equality. I enjoy breaking down complex issues into smaller steps and providing practical solutions. Finally, I commit to conscious improvement even when the model fails. I don't live up high-trade. I trade it as a chance to learn. This personal equality helps you grow faster and stay consistent in pursuing a career in AI and Engineer.\"\n",
            "          },\n",
            "          {\n",
            "            \"id\": 11,\n",
            "            \"score\": 4,\n",
            "            \"reason\": \"Comprehensive and Very Clear Response (Contains key technical terms).\",\n",
            "            \"transcript_preview\": \"Can you tell us about yourself and why you choose architecture as your role? My name is Farid Rasidin and I've always had a strong interest in how space influence people's brain experience. Architecture affects me because it brings creativity, engineering, and global improving. I enjoy analyzing how buildings function, how design affects communities, and how a technology can support sustainable development. I spent time learning architecture from conceptual, starting building, cut, soft, and understanding modern tools like CAD and 3D model. What excites me most is the idea that architectures allow us to save environments that are both beautiful and functional. I want to contribute to protect projects that make cities more available, efficient, and environmentally responsible.\"\n",
            "          },\n",
            "          {\n",
            "            \"id\": 12,\n",
            "            \"score\": 4,\n",
            "            \"reason\": \"Comprehensive and Very Clear Response (Contains key technical terms).\",\n",
            "            \"transcript_preview\": \"What architectural software are you familiar with? I am familiar with several tools commonly used in architecture world. For directing, I use AutoCAD for modeling and visual decision. I work with SketchUp and Direct Hit. And for rendering, I am learning tools like Lumion. I am also comfortable with digital drawing tablets and basic beam concepts. Beyond software, I try to focus on understanding the undrilled line principles. How to read plans, create accurate dimensions, and visualize structure in both 2D and 3D. I am still improving my speed and efficiency, but I am confident in my technology, technical foundation, and my willingness to learn additional tools depending on the project's needs.\"\n",
            "          },\n",
            "          {\n",
            "            \"id\": 13,\n",
            "            \"score\": 4,\n",
            "            \"reason\": \"Comprehensive and Very Clear Response (Contains key technical terms).\",\n",
            "            \"transcript_preview\": \"How do you approach designing a building of space? When designing a building, I begin with by understanding the purpose of the space, the users, and the environmental context. I started with sketch and simple concept to explore possible forms, followed by functions for learning and circulation flow. I consider natural light, ventilation, structure, materials, and energy efficiency. Once I have a direction, I refine the design with models and digital tools. And during professions, aesthetics, and particularly tea, stay balanced. For me, a good design must solve a real problem and improve people's daily life. I always try to create space that feels comparable, efficient, and connect to their surroundings.\"\n",
            "          },\n",
            "          {\n",
            "            \"id\": 14,\n",
            "            \"score\": 4,\n",
            "            \"reason\": \"Comprehensive and Very Clear Response (Contains key technical terms).\",\n",
            "            \"transcript_preview\": \"How do you handle design that is our multi-power efficient? I believe analytics is a natural and important part of architecture development. Designing a building is a collaborative effort involving clients, engineers, mentors, and other stakeholders. So when I receive feedback, I focus on understanding the intention behind the comments. I analyze whether the feedback improves structural feasibility, aesthetics, user experience, or sustainability. If it adds value, I refist the design carefully. I don't take the critics personally. Instead, I use to develop a more mature and well-talked design. I learned that with architecture never comes from a single idea. It evolves throughout the generations. So I remain patient, open-minded, and consistent when making revisions. I see each revision as a step toward creating a better solution that meets the needs of users and aligns with project goals.\"\n",
            "          },\n",
            "          {\n",
            "            \"id\": 15,\n",
            "            \"score\": 4,\n",
            "            \"reason\": \"Comprehensive and Very Clear Response (Contains key technical terms).\",\n",
            "            \"transcript_preview\": \"Voice will be select you for this architecture position. I believe I'm a strong fit for this position because I bring patience, dedication, and a strong will to learn. I'm deeply interested in architectural design, sustainability, and the integration of technology in modern building development. I bring a balance of creativity, analytical thinking, and discipline in how I approach every project. I'm committed to producing detailed, thoughtful, and practical design. Even though I'm still early in my career, I make up for it with my strong work ethic, adaptability, and earnest to take on challenge. I collaborate well with others, communicate clearly, and maintain a professional attitude in both fast-paced and demanding environments. I want to grow into an architect who creates space that adds value to people's lives, in-chance communities, and respect and promote sustainability. I'm confident that with my mindset and dedication, I can contribute meaningfully to your team and your future projects.\"\n",
            "          },\n",
            "          {\n",
            "            \"id\": 16,\n",
            "            \"score\": 4,\n",
            "            \"reason\": \"Comprehensive and Very Clear Response (Contains key technical terms).\",\n",
            "            \"transcript_preview\": \"Thanks for joining us. So, to start, can you just walk me through your resum\\u00e9 and tell me a little bit about yourself? Yeah, absolutely. Thanks for helping me. So, uh, I'm a final year student at the University of Indonesia, majoring in computer science with specialization in artificial intelligence. I've always been, you know, fascinated by data, and that's what led me directly into machine learning. My core experience is in, I'd say, two main projects. My capstone project, which I just finished, was an IMAGE recognition system for identifying defective products on a production line. This was a really challenging computer vision task. We had a very small dataset, so I had to heavily use data augmentation and transfer learning. We ended up fine-tuning a pre-train ResNet 50 model and achieved about 98% accuracy. On the side, I also built a personal project, a recommendation engine for local restaurants. This was more of an NLP project. Actually, I script restaurant reviews and use topic modeling, especially LDA to extract key features and then build a content-based filtering system. My last internship was at Topee, where I worked on the Delta Engineering team. This was super valuable. I learned a lot about the real world. I built data pipelines using Airflow, wrote a lot of complex SQL queries for data extraction and helped maintain their ETL processes.\"\n",
            "          },\n",
            "          {\n",
            "            \"id\": 17,\n",
            "            \"score\": 4,\n",
            "            \"reason\": \"Comprehensive and Very Clear Response (Contains key technical terms).\",\n",
            "            \"transcript_preview\": \"Thanks for looking at your projects. It clear you build a lot. Can you tell me about the single most challenging technical problem you face so far? The most challenging, yeah, I have a good one. It was definitely, definitely, definitely during my internship. We, uh, we had a machine learning model, uh, model that predicted customer turn and it worked fine in the notebook, but in the notebook, but we need to deploy it was, it was a real-time API and the, the inference speed was just terrible. It was taking, I think, three seconds to return a single prediction, which, you know, is unacceptable for a live service. My, my manager talks me with figuring out why. So first, I, I profiled the code. I took my, my, my, my pre-pre-pre-pre-processing steps where the bottleneck, I, I optimized the pandas functions, factorize everything, it got a little faster, but still not enough. The, the real issue was the model itself. It was a huge ticket learn random forest model with, I don't know, thousand of trees. So I released, I had to optimize the model itself. I researched a lot and I learned about model optimization techniques. I end up have converting the model to, to the on nnx format on nnx, the open neural network exchange. It, it basically standardize the model. Then I use the on nnx runtime for inference instead of schedule and predicts function. It was honestly a bit of gamble, but it worked. Then the, the inference time went from, from three sec, three seconds down to about 150 millisecond. It was more than 20 times faster. It was a huge win and it taught me that, that building the model is really just the first step, making it fast and deployable. That's the real challenge.\"\n",
            "          },\n",
            "          {\n",
            "            \"id\": 18,\n",
            "            \"score\": 4,\n",
            "            \"reason\": \"Comprehensive and Very Clear Response (Contains key technical terms).\",\n",
            "            \"transcript_preview\": \"Okay, that makes sense. I have one last major question for you. You're clearly a strong candidate with a lot of options. Why are you especially interested in our company Travelocca? That's actually the easiest question for me to answer. I've been following Travelocca for a while now. I'm good when only a user of your product. I use the experience feature almost every month. And as an ML student, I was always curious how it works. So I read your company's engineering blog. I read the paper your team published last year on I think it was Dynamic User State Modeling. And I was just blown away. Most companies, you know, they just use standard models. But your team is clearly building things from the ground up. You're pushing the boundaries. The blog post about your visual store infrastructure was fascinating. And that's what I want. I don't just want a job where I just call an API. I want to be in the engine room, you know. I want to be sure that by people who are smarter than me, who are solving machine scalable problems, massive scale problems, your company missions to organize information. It really resonates with me. And I truly believe my skills in data analysis and model building could actually contribute here. So yeah, that's why I'm just I'm really excited about the problems you're solving.\"\n",
            "          },\n",
            "          {\n",
            "            \"id\": 19,\n",
            "            \"score\": 4,\n",
            "            \"reason\": \"Comprehensive and Very Clear Response (Contains key technical terms).\",\n",
            "            \"transcript_preview\": \"Okay, let's move on the theme rock. Tell me about a time you had a significant disagreement with a colleague. How did you handle it? Yeah, so in my capstone group, a telecom university, we had a pretty major disagreement about project direction. I was in a team of four and we had to build an application. I was responsible for the back end and the database and one of my teammates was responsible for the front end. The disagreement it started when we were designing the API. I want to build a standard REST API, you know, with clear JSON endpoints, but my teammate, he had just learned Crab KL and he really want to use Crab KL, which arguments was that it was more efficient. You know, you only fetch the data you need. My argument was that none of us, especially me, knew it, knew it and we only had six weeks. It seemed like a huge unnecessary reach. So how did I handle it? So I saw how I did, so how did I handle it? We didn't just, you know, I scheduled a meeting just us two. And I just, I just, I listened, I let him talk for 15 minutes, explaining all the cool feature. And then I didn't say no. I said, okay, I see the benefits, but can you help me understand the reach, the risk? We made a list, the list, the risk list was really long. The learning curve, the complex server setup, we bought look at the list and he actually said, okay, you know what, maybe this isn't the right time for this. In the end, we used the rest API, but as compromised, I made API and points really clean. Exactly how his front end needs them and any work out. He respected that I listened and I respect his drive to learn new things. So just, yeah, just listening that was the key.\"\n",
            "          },\n",
            "          {\n",
            "            \"id\": 20,\n",
            "            \"score\": 4,\n",
            "            \"reason\": \"Comprehensive and Very Clear Response (Contains key technical terms).\",\n",
            "            \"transcript_preview\": \"This is great one last test. Imagine I'm not technical, I'm not an engineer. Can you explain to me in simple terms? What is transfer learning? Oh, okay. Great question. Transfer learning, right? So imagine you want to learn to play the piano, you could start from zero, learn what a key is, what a note is. It would take years to play a real song. Now imagine you're already a master at the guitar, you already understand music, you know rhythm, harmony, you don't need to learn music from scratch. You just need to learn the new part which is how do my features move on the keys. You learn piano what maybe 6 months, not 10 years. That's exactly what transfer learning is for AI. In AI, training from scratch is learning piano from zero. It takes like millions of dollars and millions of image. Transfer learning is being the bitter master. We take a master model, let's say a model called ResNet that Google or Microsoft has already trained on millions of image. That model already understands what is an edge, what is a circle, what is a texture. And then we just transfer that knowledge to our small project. For example, identifying cancer cells for research project at say the Eggman Institute, we don't need to teach the model what an edge is. We just teach it in the new part, what does a cancer cell look like. So it lets us build incredibly accurate models very fast and with very little data. It's probably the most important techniques in modern AI.\"\n",
            "          }\n",
            "        ]\n",
            "      }\n",
            "    },\n",
            "    \"Overall notes\": \"Automated assessment by AI System based on Whisper transcription.\"\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}