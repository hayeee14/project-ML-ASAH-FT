{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Proyek Capstone: AI-Powered Interview Assessment System\n",
        "**Tim A25-CS358**\n",
        "\n",
        "- **Muhammad Rayhan**, M262D5Y1357, sebagai PIC Model & Training (Streamlit/Interface)\n",
        "- **Hafiz Putra Mahesta**, M262D5Y0714, sebagai PIC Integrasi,Model STT, & Fitur (Confidence Score)\n",
        "- **Fahri Rasyidin**, M262D5Y0566, sebagai PIC Data & Evaluasi (Dataset, Kunci Jawaban, WER)"
      ],
      "metadata": {
        "id": "bfb-0BgnKO1e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Setup & Dependencies"
      ],
      "metadata": {
        "id": "9GiUsL9GLNGF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBArUUN_Tdgq"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install transformers accelerate\n",
        "!pip install jiwer moviepy librosa soundfile"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import shutil\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import moviepy.editor as mp\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import torch\n",
        "import whisper\n",
        "import jiwer\n",
        "from datetime import datetime\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import pipeline\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "V-Aa70HGjTgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "KBUpY7RV5gpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# System Configuration & Model Initialization"
      ],
      "metadata": {
        "id": "eACBEakarZVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR = \"/content/drive/MyDrive/Dataset\"\n",
        "VIDEO_INPUT_DIR = os.path.join(BASE_DIR, \"Video\")\n",
        "AUDIO_OUTPUT_DIR = os.path.join(BASE_DIR, \"Audio\")\n",
        "GROUND_TRUTH_FILE = os.path.join(BASE_DIR, \"Transkrip_Manual\")\n",
        "\n",
        "# Cek Folder Video\n",
        "if os.path.exists(VIDEO_INPUT_DIR):\n",
        "    video_files = [f for f in os.listdir(VIDEO_INPUT_DIR) if f.lower().endswith(('.mp4', '.webm', '.avi', '.mov', '.mkv'))]\n",
        "    print(f\"[INFO] Folder Video ditemukan.\")\n",
        "    print(f\"[INFO] Jumlah video yang siap diproses: {len(video_files)} file\")\n",
        "else:\n",
        "    print(f\" Folder Video TIDAK ditemukan di: {VIDEO_INPUT_DIR}\")\n",
        "\n",
        "if os.path.exists(GROUND_TRUTH_FILE):\n",
        "    print(f\"File Transkrip Manual (Kunci Jawaban) ditemukan.\")\n",
        "else:\n",
        "    print(f\"File Transkrip Manual tidak ditemukan di: {GROUND_TRUTH_FILE}\")\n",
        "\n",
        "try:\n",
        "    # Opsi lain: 'tiny', 'small', 'medium' (semakin besar semakin lambat tapi akurat)\n",
        "    model = whisper.load_model(\"small\")\n",
        "    print(\"Model Whisper berhasil dimuat ke dalam sistem.\")\n",
        "except Exception as e:\n",
        "    print(f\" Gagal memuat model. Detail error: {e}\")"
      ],
      "metadata": {
        "id": "pw5WC7Fwrgpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Core Utility Functions"
      ],
      "metadata": {
        "id": "EBBT-U9cdkem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_video_to_audio(video_path, audio_path):\n",
        "    try:\n",
        "        video_clip = mp.VideoFileClip(video_path)\n",
        "        video_clip.audio.write_audiofile(audio_path, codec='pcm_s16le', verbose=False, logger=None)\n",
        "        video_clip.close()\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def transcribe_audio(audio_path):\n",
        "    try:\n",
        "        # Menentukan konteks spesifik domain untuk meningkatkan akurasi istilah teknis.\n",
        "        technical_prompt = (\n",
        "            \"Transcribe strictly in English. Context: Machine Learning interview. \"\n",
        "            \"Keywords: TensorFlow, Scikit-learn, CNN, Dropout, Overfitting, Transfer Learning. \"\n",
        "            \"Do not include filler words like umm, uh, ah.\"\n",
        "        )\n",
        "\n",
        "        result = model.transcribe(\n",
        "            audio_path,\n",
        "            fp16=False,\n",
        "            language=\"en\",\n",
        "            initial_prompt=technical_prompt\n",
        "        )\n",
        "        return result[\"text\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\" Transkripsi Error: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def remove_fillers(text):\n",
        "    # Daftar kata filler yang mau dihapus (bisa ditambah)\n",
        "    fillers = [\n",
        "        r\"\\bum\\b\", r\"\\buh\\b\", r\"\\buhh\\b\", r\"\\bah\\b\", r\"\\ber\\b\", r\"\\bhmm\\b\",\n",
        "        r\"\\bmhm\\b\", r\"\\buh-huh\\b\", r\"\\bokay\\b\",\n",
        "        r\"\\byou know\\b\", r\"\\bi mean\\b\", r\"\\bkind of\\b\", r\"\\bsort of\\b\",\n",
        "        r\"\\bso\\b\", r\"\\blike\\b\", r\"\\byeah\\b\", r\"\\bright\\b\",\n",
        "    ]\n",
        "\n",
        "    clean_text = text.lower()\n",
        "    for filler in fillers:\n",
        "        clean_text = re.sub(filler, \"\", clean_text)\n",
        "\n",
        "    # Hapus spasi ganda sisa penghapusan\n",
        "    clean_text = re.sub(r'\\s+', ' ', clean_text).strip()\n",
        "    return clean_text\n",
        "\n",
        "def calculate_metrics(reference_text, hypothesis_text):\n",
        "    if not reference_text or not hypothesis_text:\n",
        "        return {\"wer\": 1.0, \"accuracy\": 0.0}\n",
        "\n",
        "    # Bersihkan tanda baca dasar\n",
        "    transformation = jiwer.Compose([\n",
        "        jiwer.ToLowerCase(),\n",
        "        jiwer.RemovePunctuation(),\n",
        "        jiwer.RemoveMultipleSpaces(),\n",
        "        jiwer.Strip(),\n",
        "    ])\n",
        "\n",
        "    ref_basic = transformation(reference_text)\n",
        "    hyp_basic = transformation(hypothesis_text)\n",
        "\n",
        "    # Hapus filler words\n",
        "    ref_clean = remove_fillers(ref_basic)\n",
        "    hyp_clean = remove_fillers(hyp_basic)\n",
        "\n",
        "    # Hitung Akurasi\n",
        "    wer_score = jiwer.wer(ref_clean, hyp_clean)\n",
        "    accuracy = max(0, 1 - wer_score) * 100\n",
        "\n",
        "    return {\"wer\": wer_score, \"accuracy\": round(accuracy, 2)}"
      ],
      "metadata": {
        "id": "pqj4uUmUdpW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Processing Pipeline (ETL)"
      ],
      "metadata": {
        "id": "efs9kC-we0a5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRANSCRIPT_DIR = os.path.join(BASE_DIR, \"Transkrip_Manual\")\n",
        "\n",
        "# Ekstraksi Audio (Video -> Audio)\n",
        "video_files = [f for f in os.listdir(VIDEO_INPUT_DIR) if f.lower().endswith(('.mp4', '.avi', '.webm'))]\n",
        "print(f\"Memulai pemrosesan untuk {len(video_files)} file video...\")\n",
        "\n",
        "for video in tqdm(video_files, desc=\"Converting Videos\"):\n",
        "    v_path = os.path.join(VIDEO_INPUT_DIR, video)\n",
        "    a_path = os.path.join(AUDIO_OUTPUT_DIR, os.path.splitext(video)[0] + \".wav\")\n",
        "\n",
        "    if not os.path.exists(a_path):\n",
        "        convert_video_to_audio(v_path, a_path)\n",
        "\n",
        "# Mengumpulkan semua file audio dari hasil konversi maupun yang sudah ada di folder\n",
        "all_audio_files = []\n",
        "\n",
        "for f in os.listdir(AUDIO_OUTPUT_DIR):\n",
        "    if f.lower().endswith('.wav'):\n",
        "        all_audio_files.append(os.path.join(AUDIO_OUTPUT_DIR, f))\n",
        "\n",
        "# Menghapus duplikasi path jika ada\n",
        "all_audio_files = list(set(all_audio_files))\n",
        "print(f\"Total file audio yang siap diproses: {len(all_audio_files)} file\")\n",
        "\n",
        "# Proses Transkripsi & Evaluasi\n",
        "processing_results = []\n",
        "total_accuracy = 0\n",
        "count_evaluated = 0\n",
        "\n",
        "for audio_path in tqdm(all_audio_files, desc=\"AI Transcribing\"):\n",
        "    filename = os.path.basename(audio_path)\n",
        "    base_name = os.path.splitext(filename)[0]\n",
        "\n",
        "    pred_text = transcribe_audio(audio_path)\n",
        "\n",
        "    metrics = {\"accuracy\": 0.0}\n",
        "    truth_text = \"N/A\"\n",
        "\n",
        "    txt_path = os.path.join(TRANSCRIPT_DIR, base_name + \".txt\")\n",
        "\n",
        "    if os.path.exists(txt_path):\n",
        "        try:\n",
        "            with open(txt_path, 'r', encoding='utf-8') as f:\n",
        "                truth_text = f.read().strip()\n",
        "            # Hitung akurasi\n",
        "            metrics = calculate_metrics(truth_text, pred_text)\n",
        "            total_accuracy += metrics[\"accuracy\"]\n",
        "            count_evaluated += 1\n",
        "        except: pass\n",
        "\n",
        "    # Simpan hasil ke list\n",
        "    processing_results.append({\n",
        "        \"filename\": filename,\n",
        "        \"prediction\": pred_text,\n",
        "        \"ground_truth\": truth_text[:100],\n",
        "        \"accuracy\": metrics[\"accuracy\"]\n",
        "    })\n",
        "\n",
        "# Laporan Ringkasan\n",
        "print(\"-\" * 50)\n",
        "df_results = pd.DataFrame(processing_results)\n",
        "\n",
        "if count_evaluated > 0:\n",
        "    avg_accuracy = total_accuracy / count_evaluated\n",
        "else:\n",
        "    avg_accuracy = 0.0\n",
        "\n",
        "# Tampilkan statistik akhir\n",
        "print(f\"Total Data Diproses       : {len(processing_results)}\")\n",
        "print(f\"Data dengan Kunci Jawaban : {count_evaluated}\")\n",
        "print(f\"Rata-rata Akurasi         : {avg_accuracy:.2f}%\")\n",
        "\n",
        "# Validasi terhadap target keberhasilan proyek\n",
        "if avg_accuracy >= 90:\n",
        "    print(\"STATUS: TERCAPAI\")\n",
        "else:\n",
        "    print(\"STATUS: BELUM TERCAPAI\")\n",
        "\n",
        "df_results[[\"filename\", \"accuracy\"]]"
      ],
      "metadata": {
        "id": "QkMpnuiWe2GM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI-Powered Assessment Engine (LLM Reasoning) dan Final Reporting & Export"
      ],
      "metadata": {
        "id": "xfYF2PONqiO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Konfigurasi Model Penilai (LLM)\n",
        "# Menggunakan Google Flan-T5 Large untuk memberikan skor dan alasan penilaian\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "assessor_llm = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=\"google/flan-t5-large\",\n",
        "    device=device,\n",
        "    max_length=512\n",
        ")\n",
        "print(f\"Model LLM berhasil dimuat pada device: {device}\")\n",
        "\n",
        "# Basis Pengetahuan Pertanyaan\n",
        "# Digunakan untuk memberikan konteks kepada AI saat menilai jawaban\n",
        "QUESTION_DB = {\n",
        "    1: \"Share specific challenges you faced in certification and how you overcame them.\",\n",
        "    2: \"Describe your experience with transfer learning in TensorFlow.\",\n",
        "    3: \"Describe a complex TensorFlow model you built and how you ensured accuracy.\",\n",
        "    4: \"Explain how to implement dropout and its effect on training.\",\n",
        "    5: \"Describe the process of building a CNN for image classification.\",\n",
        "    6: \"Tell us about yourself and why you want to become an AI Engineer.\",\n",
        "    7: \"What AI/ML tools or skills have you learned so far?\",\n",
        "    8: \"Explain what Machine Learning is in simple words.\",\n",
        "    9: \"How do you solve problems when your model does not work well?\",\n",
        "    10: \"What personal qualities do you have to support you as an AI Engineer?\",\n",
        "    11: \"Why did you choose architecture as your field?\",\n",
        "    12: \"What architectural software are you familiar with?\",\n",
        "    13: \"How do you approach designing a building or space?\",\n",
        "    14: \"How do you handle design criticism or multiple revisions?\",\n",
        "    15: \"Why should we select you for this position?\",\n",
        "    16: \"Walk me through your resume and tell me about yourself.\",\n",
        "    17: \"Tell me about the most challenging technical problem you've faced.\",\n",
        "    18: \"Why are you specifically interested in our company (Traveloka)?\",\n",
        "    19: \"Tell me about a time you had a significant disagreement with a colleague.\",\n",
        "    20: \"Explain Transfer Learning to a non-technical person.\"\n",
        "}\n",
        "\n",
        "# Logika Penilaian Utama\n",
        "def assess_with_llm(answer_text, question_id):\n",
        "    # Validasi jawaban yang terlalu pendek\n",
        "    if not answer_text or len(answer_text) < 10:\n",
        "        return 0, \"Candidate did not provide a meaningful answer.\"\n",
        "\n",
        "    question_text = QUESTION_DB.get(question_id, \"General interview question\")\n",
        "\n",
        "    # Instruksi (Prompt) untuk LLM agar bertindak sebagai Recruiter\n",
        "    prompt = f\"\"\"\n",
        "    You are a Senior Technical Recruiter. Evaluate this interview answer.\n",
        "\n",
        "    Question: \"{question_text}\"\n",
        "    Candidate Answer: \"{answer_text}\"\n",
        "\n",
        "    Task:\n",
        "    1. Give a Score (1-4) based on depth, clarity, and technical correctness.\n",
        "    2. Write a short Reason (max 1 sentence).\n",
        "\n",
        "    Rubric:\n",
        "    1: Poor/Irrelevant.\n",
        "    2: Basic/General.\n",
        "    3: Good/Specific.\n",
        "    4: Excellent/Detailed.\n",
        "\n",
        "    Output Format: Score | Reason\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Menjalankan inferensi model\n",
        "        output = assessor_llm(prompt, max_length=128, do_sample=False)[0]['generated_text']\n",
        "\n",
        "        # Memisahkan skor dan alasan dari teks output model\n",
        "        if \"|\" in output:\n",
        "            parts = output.split(\"|\", 1)\n",
        "            score_str = re.search(r'\\d+', parts[0])\n",
        "            score = int(score_str.group()) if score_str else 2\n",
        "            reason = parts[1].strip()\n",
        "        else:\n",
        "            # Nilai default jika format output tidak sesuai\n",
        "            score = 3\n",
        "            reason = output\n",
        "\n",
        "        return max(1, min(4, score)), reason\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Gagal menilai pertanyaan ID {question_id}: {e}\")\n",
        "        return 2, \"AI could not process this specific answer.\"\n",
        "\n",
        "def extract_id_from_filename(filename):\n",
        "    # Mengambil angka dari nama file untuk keperluan pengurutan\n",
        "    numbers = re.findall(r'\\d+', filename)\n",
        "    return int(numbers[0]) if numbers else 999\n",
        "\n",
        "# Pembuatan Laporan Akhir\n",
        "def generate_final_report(results_data):\n",
        "    # Struktur data JSON disesuaikan dengan kebutuhan sistem backend (payload)\n",
        "    final_output = {\n",
        "        \"success\": True,\n",
        "        \"data\": {\n",
        "            \"id\": 131,\n",
        "            \"candidate\": {\n",
        "                \"name\": \"Hafiz Putra Mahesta\",\n",
        "                \"email\": \"hafiz123@gmail.com\",\n",
        "                \"photoUrl\": \"https://path/to/photo.png\"\n",
        "            },\n",
        "            \"certification\": {\n",
        "                \"normalType\": \"DEV_CERTIFICATION_MACHINE_LEARNING\",\n",
        "                \"status\": \"FINISHED\"\n",
        "            },\n",
        "            \"pastReviews\": []\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Objek untuk menyimpan hasil review saat ini\n",
        "    current_review = {\n",
        "        \"assessorProfile\": {\n",
        "            \"id\": 47,\n",
        "            \"name\": \"XXX\",\n",
        "            \"photoUrl\": \"XXX\"\n",
        "        },\n",
        "        \"decision\": \"PENDING\",\n",
        "        \"reviewedAt\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "        \"scoresOverview\": {\"project\": 100, \"interview\": 0, \"total\": 0},\n",
        "        \"reviewChecklistResult\": {\n",
        "            \"project\": [],\n",
        "            \"interviews\": {\"minScore\": 0, \"maxScore\": 4, \"scores\": []}\n",
        "        },\n",
        "        \"notes\": \"Automated assessment utilizing OpenAI Whisper for transcription and Google Flan-T5 for reasoning.\"\n",
        "    }\n",
        "\n",
        "    total_score = 0\n",
        "    count = 0\n",
        "\n",
        "    # Mengurutkan data berdasarkan ID pertanyaan\n",
        "    sorted_results = sorted(results_data, key=lambda x: extract_id_from_filename(x['filename']))\n",
        "\n",
        "    print(f\"Memulai penilaian otomatis untuk {len(sorted_results)} jawaban...\")\n",
        "\n",
        "    for item in tqdm(sorted_results, desc=\"Penilaian AI\"):\n",
        "        filename = item['filename']\n",
        "        q_id = extract_id_from_filename(filename)\n",
        "\n",
        "        # Proses penilaian menggunakan LLM\n",
        "        score, reason = assess_with_llm(item['prediction'], q_id)\n",
        "\n",
        "        checklist_item = {\n",
        "            \"id\": q_id,\n",
        "            \"score\": score,\n",
        "            \"reason\": reason,\n",
        "            \"transcript_preview\": item['prediction']\n",
        "        }\n",
        "\n",
        "        current_review[\"reviewChecklistResult\"][\"interviews\"][\"scores\"].append(checklist_item)\n",
        "\n",
        "        if 1 <= q_id <= 20:\n",
        "            total_score += score\n",
        "            count += 1\n",
        "\n",
        "    # Menghitung skor akhir interview\n",
        "    if count > 0:\n",
        "        interview_score = (total_score / (count * 4)) * 100\n",
        "    else:\n",
        "        interview_score = 0\n",
        "\n",
        "    current_review[\"scoresOverview\"][\"interview\"] = round(interview_score, 2)\n",
        "\n",
        "    # Menghitung skor total gabungan\n",
        "    project_score = 100\n",
        "    total_final = (project_score + interview_score) / 2\n",
        "    current_review[\"scoresOverview\"][\"total\"] = round(total_final, 2)\n",
        "\n",
        "    # Menentukan keputusan akhir\n",
        "    current_review[\"decision\"] = \"PASSED\" if total_final >= 75 else \"NEED REVISION\"\n",
        "\n",
        "    # Menggabungkan hasil review ke struktur utama\n",
        "    final_output[\"data\"][\"pastReviews\"].append(current_review)\n",
        "\n",
        "    return final_output\n",
        "\n",
        "# Eksekusi dan Ekspor Data\n",
        "# Memastikan variabel hasil transkripsi tersedia sebelum melanjutkan\n",
        "if 'processing_results' in locals() and processing_results:\n",
        "    json_report = generate_final_report(processing_results)\n",
        "\n",
        "    # Menyimpan hasil ke file JSON\n",
        "    output_path = os.path.join(BASE_DIR, \"final_assessment_result.json\")\n",
        "    with open(output_path, \"w\") as f:\n",
        "        json.dump(json_report, f, indent=2)\n",
        "\n",
        "    print(f\"Laporan berhasil dibuat dan disimpan di: {output_path}\")\n",
        "else:\n",
        "    print(\"Data transkripsi tidak ditemukan. Pastikan proses ETL sudah dijalankan.\")"
      ],
      "metadata": {
        "id": "xfA18IRoqj0f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}